[
  {
    "objectID": "blog/causal-inference.html",
    "href": "blog/causal-inference.html",
    "title": "Part VII: Key concepts in causal inference",
    "section": "",
    "text": "This is part seven of a series on statistical methods for analysing time-to-event, or “survival” data.\nIn this post I’ll introduce the concept of causal inference from observational data and provide an outline of three common methods to compute the average causal effect."
  },
  {
    "objectID": "blog/causal-inference.html#assessing-causal-effects",
    "href": "blog/causal-inference.html#assessing-causal-effects",
    "title": "Part VII: Key concepts in causal inference",
    "section": "Assessing causal effects",
    "text": "Assessing causal effects\nIn trial settings, well-designed randomised experiments, where the treated and untreated are theoretically exchangeable (or exogenous), provide a rigorous way to assess causal effects within a specific population. Under these ideal conditions, any observed association between the treatment and the outcome can be more confidently interpreted as causation, due to the minimisation of confounding and selection bias. However, even randomised trials have limitations, with potential issues including external validity or the Hawthorne effect - where participants modify their behaviour in response to being observed.\nIn contrast, observational data typically present more challenges in establishing causality due to the potential for unmeasured confounding and selection bias. If an explicit causal question is defined (e.g. the effect of a well-specified hypothetical intervention), and certain stringent conditions fulfilled (such as no unmeasured confounding, positivity, and consistency) it becomes possible to make causal inferences from observational data. This approach, referred to as the potential outcome framework, requires careful consideration of the study design, choice of appropriate statistical methods, and transparent reporting of assumptions, with any causal claims depending on the plausibility of these assumptions."
  },
  {
    "objectID": "blog/causal-inference.html#causal-diagrams",
    "href": "blog/causal-inference.html#causal-diagrams",
    "title": "Part VII: Key concepts in causal inference",
    "section": "Causal diagrams and local independence",
    "text": "Causal diagrams and local independence\nCausal diagrams are a graphical tool used to visualise causal relationships. One class of causal diagrams are causal directed acyclic graphs (causal DAGs). In these graphs, nodes represent measurements or interventions at discrete times, and edges between nodes indicate causal effects, an example is shown in Figure 1. The lack of an edge between nodes can be interpreted as the absence of a direct effect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Example of a causal DAG representing discrete measurements for treatment, \\(A\\), causes of treatment, \\(C\\), and outcome, \\(Y\\).\n\n\n\nWhilst causal DAGs are useful to describe relationships between discrete-time processes, they are poorly suited to describing causality in continuous-time, as changes in quantities or the occurrence of events are not represented. Instead, for continuous-time processes, the concept of local independence may be applied to consider how well a future value of an intensity process (such as a transition intensity in a multi-state model) is predicted by the past. Local independence can be viewed as immediate causation, and can be represented in a type of causal diagram known as a local independence graph. In these graphs, nodes represent stochastic processes and edges between nodes represent immediate causal influence, example shown in Figure 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Example of a local independence graph representing continuous processes for treatment, \\(A\\), causes of treatment, \\(C\\), and outcome, \\(Y\\). In local independence graphs only immediate causal influences are shown."
  },
  {
    "objectID": "blog/causal-inference.html#causal-inference",
    "href": "blog/causal-inference.html#causal-inference",
    "title": "Part VII: Key concepts in causal inference",
    "section": "Causal inference from observational data",
    "text": "Causal inference from observational data\nTo make causal inference from observational data we rely on being able to analyse the data as if treatment had been randomly assigned, conditional on a set of measured covariates \\(Z\\). This is termed a conditionally randomised experiment.\nLet \\(Y = (0:\\) doesn’t experience outcome, \\(1:\\) experiences outcome\\()\\) be a random variable for an observed outcome, and \\(A = (0:\\) untreated, \\(1:\\) treated\\()\\) be a random variable for an observed treatment, then:\n\\[\n\\Pr(Y = 1 \\mid A = 1)\n\\]\nis the probability of experiencing the outcome given treatment. Define the potential outcome under treatment \\(a\\) as \\(Y^a\\), then the potential outcome under the observed treatment is: \\(Y^A = Y\\).\n\nIdentifiability conditions\nThree identifiability conditions are required for an observational study to be treated as a conditionally randomised experiment:\n\nConsistency: the values of treatment under comparison correspond to well-defined interventions that, in turn, correspond to the versions of treatment in the data, i.e. \\(Y^a = Y\\) for individuals with \\(A = a\\).\nExchangeability: the conditional probability of receiving every value of treatment depends only on measured covariates \\(Z\\). This is alternatively phrased as: conditional on a set of measured covariates \\(Z\\), the untreated group, had they been treated, would experience the same average outcome as the treated group, i.e.\n\n\\[\n\\Pr(Y^a = 1 \\mid A = 1, Z = z) = \\Pr(Y^a = 1 \\mid A = 0, Z = z)\n\\]\nor equivalently, \\(Y^a \\mathrel{\\perp\\!\\!\\!\\!\\perp} A \\mid Z = z\\), the counterfactual outcome and the observed treatment are independent within the covariate levels \\(Z = z\\).\n\nPositivity: the probability of receiving every value of treatment \\(a\\) conditional on \\(Z = z\\) is positive, i.e.\n\n\\[\n\\Pr(A = a \\mid Z = z) &gt; 0 \\quad\\text{for all } z \\text{ where } \\Pr(Z = z) \\neq 0\n\\]\nAmong these conditions, consistency and positivity are usually straightforward to check in observational studies whilst exchangeability relies on the assumption that all predictors of an outcome have been measured. It is usually impossible to guarantee exchangeability, but understanding the potential for biases such as confounding and collider bias (defined below) can inform which covariates to include such that the assumption will be approximately true.\nIf the three identifiability conditions are fulfilled, a (hypothetical) randomised experiment, known as a target trial, may be emulated using causal inference from observational data, with the average causal effect quantified as \\(E(Y^{a = 1}) - E(Y^{a = 0})\\)."
  },
  {
    "objectID": "blog/causal-inference.html#causal-methods",
    "href": "blog/causal-inference.html#causal-methods",
    "title": "Part VII: Key concepts in causal inference",
    "section": "Methods to compute the average causal effect",
    "text": "Methods to compute the average causal effect\nThree commonly-used methods to compute the average causal effect are inverse probability weighting, matching, and standardisation.\n\nInverse probability weighting\nThe concept of inverse probability weighting is to assign weights to each individual in the cohort based on the inverse of the probability of receiving the observed treatment level, conditional on their covariates, \\(Z\\). For example, a treated individual, \\(A = 1\\), with a set of covariates \\(Z = z\\) would be assigned a weight of \\(1/\\Pr(A = 1 \\mid Z = z)\\).\nConversely, an untreated individual, \\(A = 0\\), with covariates \\(Z = z'\\) would be assigned a weight of \\(1/\\Pr(A = 0 \\mid Z = z')\\). These weights are used to adjust for potential baseline confounders when estimating the causal effect of treatment, allowing the creation of a pseudo-population where treatment assignment is independent of the observed covariates.\n\n\nMatching\nMatching involves constructing a subset of the population in which the distribution of covariates, \\(Z\\), is the same for the treated and untreated groups. This is typically achieved by pairing each treated individual with an untreated individual with the same or similar covariate values. The causal effect can be estimated as the average difference in outcomes among the matched pairs.\n\n\nStandardization\nStandardisation involves estimating the counterfactual risk using a weighted average of the risks (or standardisation of the risks) in each covariate level. For example, for a covariate \\(Z\\) with two levels \\((0,1)\\):\n\\[\n\\Pr(Y^a = 1) = \\Pr(Y = 1 \\mid Z = 1, A = a)\\Pr(Z = 1) + \\Pr(Y = 1 \\mid Z = 0, A = a)\\Pr(Z = 0)\n\\]\nwhich generalises to: \\[\n\\Pr(Y^a = 1) = \\sum_z{\\Pr(Y = 1 \\mid Z = z, A = a)\\Pr(Z = z)}\n\\]\nThe standardised mean \\(E(Y^a) = \\sum_z{E(Y \\mid Z = z, A = a)\\Pr(Z = z)}\\) is known as the g-formula.\n\n\n\n\n\n\nNoteKey takeaways\n\n\n\n\nRandomised trials provide strong evidence for causation, but observational data can also support causal inference under strict conditions\nCausal diagrams (DAGs) help visualise relationships between variables, while local independence graphs represent continuous-time causal processes\nThree key conditions enable causal inference from observational data: consistency, exchangeability, and positivity\nExchangeability (no unmeasured confounding) is usually the most challenging assumption to verify in practice\nThree main methods estimate causal effects: inverse probability weighting, matching, and standardisation (g-formula)\nEach method creates a “pseudo-population” where treatment assignment becomes independent of confounders"
  },
  {
    "objectID": "blog/causal-inference.html#coming-next",
    "href": "blog/causal-inference.html#coming-next",
    "title": "Part VII: Key concepts in causal inference",
    "section": "Coming next",
    "text": "Coming next\nIn the final post of this series, I’ll explore different observational study designs and several of the biases that can affect inference in observational research: information bias, collider bias, confounding, and epidemic phase bias."
  },
  {
    "objectID": "blog/causal-inference.html#references",
    "href": "blog/causal-inference.html#references",
    "title": "Part VII: Key concepts in causal inference",
    "section": "References",
    "text": "References\n\nAalen OO, Røysland K, Gran JM, Kouyos R et al. Can we believe the DAGs? A comment on the relationship between causal DAGs and mechanisms. Stat. Methods Med. Res. 2016; 25(5), pp. 2294-2314.\nDidelez V. Graphical Models for Marked Point Processes Based on Local Independence. J. R. Stat. Soc. Series B Stat. Methodol. 2008; 70(1), pp.245-264.\nGriffith GJ, Morris TT, Tudball MJ, et al. Collider bias undermines our understanding of COVID-19 disease risk and severity. Nature Communications. 11(1),5749.\nHernán MA, Robins JM. Estimating causal effects from epidemiological data. J. Epidemiol. Community Health. 2006;60(7), pp.578-586.\nHernán MA, Robins JM. Causal Inference: What If. CRC Press; 2023, 312 pp.\nMcCambridge J, Witton J, Elbourne DR. Systematic review of the Hawthorne effect: new concepts are needed to study research participation effects. J. Clin. Epidemiol. 2014;67(3),pp.267-277.\n\n\n← Previous post\nNext post →"
  },
  {
    "objectID": "blog/statistical-inference-msms.html",
    "href": "blog/statistical-inference-msms.html",
    "title": "Part VI: Statistical inference for multi-state models",
    "section": "",
    "text": "This is part six of a series on statistical methods for analysing time-to-event, or “survival” data.\nIn this post I’ll describe two statistical inference methods (non-parametric and parametric multi-state survival models). I discuss how these multi-state methods are well-suited to the investigation of intermittently-observed data."
  },
  {
    "objectID": "blog/statistical-inference-msms.html#aalen-johansen",
    "href": "blog/statistical-inference-msms.html#aalen-johansen",
    "title": "Part VI: Statistical inference for multi-state models",
    "section": "Aalen-Johansen estimator",
    "text": "Aalen-Johansen estimator\nThe non-parametric Aalen-Johansen estimator can be used to estimate the transition probability matrix, \\(\\mathbf{P}(t)\\) for a Markov process with a finite number of states. Firstly, let \\(\\mathbf{A}(t)\\) be the matrix of cumulative transition intensities from state \\(r\\) to state \\(s\\):\n\\[\n\\mathbf{A}(t) = A_{r,s}(t) =\n\\begin{cases}\n\\int_{0}^{t}{q_{r,s}(u) du} & \\text{if } r\\neq s \\\\\n-\\sum_{r \\neq s}{A_{r,s}(t)} & \\text{if } r = s\n\\end{cases}\n\\]\nThe transition probability matrix for this process can be represented as:\n\\[\n\\mathbf{P} = \\prod_{k = 1}^{K}{\\left(\\mathbb{I}+\\Delta\\mathbf{A}(t_k)\\right)}\n\\]\nwhere \\(\\mathbb{I}\\) is the identity matrix, \\(\\Delta\\mathbf{A}(t_k) = \\mathbf{A}(t_k) - \\mathbf{A}(t_k \\mbox{-})\\), and \\(t_k \\mbox{-}\\) represents the instant before time \\(t_k\\). Substituting the Nelson-Aalen estimator for \\(\\mathbf{A}(t)\\), then the Aalen-Johansen estimate of the transition probability matrix is:\n\\[\n\\mathbf{{\\hat{P}}}(t) = \\hat{P}_{r,s}(t) = \\prod_{k =\n1}^{K}{\\left(\\mathbb{I}_{r,s}+\\Delta\\hat{A}_{r,s}(t_k)\\right)}\n\\]\nwith:\n\\[\n\\Delta\\hat{A}_{r,s}(t_k) =\n\\begin{cases}\n\\frac{\\Delta N_{r,s}(t_k)}{Y_{r}(t_k \\mbox{-})} & \\text{if } r\\neq s \\\\\n\\frac{-\\Delta N_{s}(t_k)}{Y_{s}(t_k \\mbox{-})} & \\text{if } r = s\n\\end{cases}\n\\]\nwhere \\(\\Delta N_{r,s}(t_k)\\) is the number of transitions from state \\(r\\) to state \\(s\\) at time \\(t_k\\), \\(\\Delta N_{s}(t_k)\\) is the number of transitions away from state \\(s\\) at time \\(t_k\\), and \\(Y_{r}(t_k \\mbox{-})\\) is the number of individuals in state \\(r\\) just before time \\(t_k\\)."
  },
  {
    "objectID": "blog/statistical-inference-msms.html#multi-state-mixture-models",
    "href": "blog/statistical-inference-msms.html#multi-state-mixture-models",
    "title": "Part VI: Statistical inference for multi-state models",
    "section": "Multi-state mixture models",
    "text": "Multi-state mixture models\nFor a general multi-state competing risks mixture model, assume that an individual \\(i\\) who begins in state \\(r\\) makes a transition to a (pre-assigned) destination state \\(s\\). Let \\(I_{i,r}\\) be the indicator variable which determines which transition will occur, the transition intensity at time \\(t\\) is:\n\\[\nq_{i,r,s}(t) =\n\\begin{cases}\nq^{*}_{i,r,s}(t) & \\text{if } I_{i,r} = s \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nwhere \\(q^{*}_{i,r,s}(t)\\) is the transition intensity for the transition which occurs. The probability of transition to state \\(s\\) is defined as:\n\\[\n\\begin{align*}\n\\pi_{r,s} & = \\Pr(I_{i,r} = s)\n\\end{align*}\n\\]\nLet \\(T_{r,s}\\) be the the time from entering state \\(r\\) to moving to state \\(s\\), given that this transition occurs (i.e. the conditional sojourn time). A parametric distribution with parameters \\(\\theta_{r,s}\\) and conditional density \\(f_{r,s}(. \\mid \\theta_{r,s})\\) may be used to model \\(T_{r,s}\\).\n\nTransition probabilities\nThe probabilities of competing events \\(\\pi_{r,s}\\) can be related to a set of \\(m\\) covariates \\(\\boldsymbol{z} = \\{z_1, \\ldots, z_m\\}\\) via multinomial logistic regression. Define as \\(S_r\\) the set of all competing states after \\(r\\), then the logit of transition to a given state \\(j \\in S_r\\) vs. transition to a baseline state \\(0 \\in S_r\\) is:\n\\[\n\\ln\\left(\\frac{\\pi_{r,j}(\\boldsymbol{z})}{\\pi_{r,0}(\\boldsymbol{z})}\\right) = \\alpha_{r,j} +\n\\boldsymbol{\\beta}_{r,j}^\\mathsf{T} \\boldsymbol{z}\n\\]\nwith \\(\\alpha_{r,j}\\) and \\(\\boldsymbol{\\beta}_{r,j} = \\{\\beta_{r,j,k};\\; k = 1, \\ldots m\\}\\) being regression coefficients for the probability of transition to state \\(j\\).\nMeanwhile, parameters \\(\\theta_{r,s}\\) of the time to transition distribution can be related to a different (or identical) set of \\(l\\) covariates \\(\\boldsymbol{z} = \\{z_1, \\ldots, z_l\\}\\) via a log-linear model with coefficients \\(\\gamma_{r,s}\\) and \\(\\boldsymbol{\\omega}_{r,s} = \\{\\omega_{r,s,k};\\; k = 1, \\ldots, l\\}\\):\n\\[\n\\log(\\theta_{r,s}(\\boldsymbol{z}))=\\gamma_{r,s} + \\boldsymbol{\\omega}_{r,s}^\\mathsf{T} \\boldsymbol{z}\n\\]\nFor most parametric time to transition distributions `accelerated failure time’ models are considered, where the covariates affect the rate of progression of time, i.e. the survival function \\(S(t \\mid \\boldsymbol{z})\\) for an individual at time \\(t\\) with covariates \\(\\boldsymbol{z}\\) is related to the baseline survival \\(S^{(0)}\\) via:\n\\[\nS(t\\mid \\boldsymbol{z}) = S^{(0)}\\left(\\exp(\\boldsymbol{\\theta}^\\mathsf{T}\\boldsymbol{z})t\\right)\n\\]\nwhere \\(\\boldsymbol{\\theta}\\) is a vector of regression coefficients.\n\n\nLikelihood contributions\nThe likelihood contributions are as follows:\n\nFor an individual who reaches state \\(s\\) at time \\(t\\): \\[\nl_i = \\pi_i f(t\\mid i=s)\n\\]\nFor an individual who does not reach a state (e.g. remains in hospital) at time \\(t\\): \\[\nl_c = \\sum_{i=1}^k\\pi_i(1-F(t\\mid i))\n\\]\n\nwhere \\(F(t\\mid i) = \\int_0^t f(x\\mid i)dx\\) is the conditional cumulative distribution."
  },
  {
    "objectID": "blog/statistical-inference-msms.html#accounting-for-censoring",
    "href": "blog/statistical-inference-msms.html#accounting-for-censoring",
    "title": "Part VI: Statistical inference for multi-state models",
    "section": "Accounting for censoring",
    "text": "Accounting for censoring\nAs described in part III of this series, survival models may be specified to account for censoring and truncation in observed data by careful consideration and construction of the likelihood function. Multi-state survival models, in particular, are well-suited to investigating interval-censored, or intermittently-observed data.\nIn Figure 2 from part II of this series, changes in an underlying process were detected with an intermittent observation scheme. Depending on the nature of the underlying process, less frequent and/or irregular observation may result in transitions being undetected, as shown in Figure 1 below. This could be an issue for survival models, particularly when there is a dependency between the observation times and the underlying process.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Example of intermittently-observed data with missed observations for an underlying process.\n\n\n\nAn observation scheme is said to be non-informative if the likelihood is proportional to a scenario where observation times were fixed in advance and chosen independently of the underlying process. With a non-informative observation scheme then multi-state models can correctly account for intermittently-observed data."
  },
  {
    "objectID": "blog/statistical-inference-msms.html#implementing-multi-state-models",
    "href": "blog/statistical-inference-msms.html#implementing-multi-state-models",
    "title": "Part VI: Statistical inference for multi-state models",
    "section": "Implementing multi-state models",
    "text": "Implementing multi-state models\nA number of sophisticated statistical software packages exist to implement frequentist and Bayesian survival and multi-state models using the R programming language. Frequentist maximum likelihood estimation tends to be less computationally intensive than Bayesian inference, and the latter may require integrating over a vast number of parameters, which is usually impracticable for higher dimensions. Instead, algorithms which simulate from the posterior distribution are often used to evaluate features of these integrals. Markov Chain Monte Carlo (MCMC) algorithms have been widely used for this application.\nThere are several specialised modelling packages available to fit multi-state models, including: survival, msm, flexsurv which fit models using maximum likelihood, and JAGS and STAN which fit Bayesian models using MCMC.\nTwo useful online tutorials for running statistical inference for survival and multi-state models are:\n\nSurvival analysis in R” by Emily C. Zabor\nMulti-state modelling with msm: a practical course by Christopher Jackson\n\n\n\n\n\n\n\nNoteKey takeaways\n\n\n\n\nThe Aalen-Johansen estimator extends the Kaplan-Meier approach to provide non-parametric estimates of transition probabilities in multi-state models\nMulti-state mixture models allow for different transition types with their own probability distributions and covariate effects\nIntermittently-observed data with interval censoring can be properly handled by multi-state models when observation schemes are non-informative\nCovariates can influence both the probability of transitions between states and the timing of those transitions\nBoth frequentist (maximum likelihood) and Bayesian (MCMC) approaches are available for fitting multi-state models, with several specialised R packages providing implementation"
  },
  {
    "objectID": "blog/statistical-inference-msms.html#coming-next",
    "href": "blog/statistical-inference-msms.html#coming-next",
    "title": "Part VI: Statistical inference for multi-state models",
    "section": "Coming next",
    "text": "Coming next\nIn the next post I’ll introduce the concept of causal inference from observational data and provide an outline of three common methods to compute the average causal effect."
  },
  {
    "objectID": "blog/statistical-inference-msms.html#references",
    "href": "blog/statistical-inference-msms.html#references",
    "title": "Part VI: Statistical inference for multi-state models",
    "section": "References",
    "text": "References\n\nAndersen PK, Borgan O, Gill RD, Keiding N. Statistical models based on counting processes. Springer; 1996. 784 p.\nFagerland MW, Hosmer DW, Bofin AM. Multinomial goodness-of-fit tests for logistic regression models. Stat Med. 2008 Sep 20 [cited 2024 Feb 17];27(21):4238-53.\nGhani AC, Donnelly CA, Cox DR, et al. Methods for estimating the case fatality ratio for a novel, emerging infectious disease. Am J Epidemiol. 2005 Sep 1;162(5):479-86.\nGrüger J, Kay R, Schumacher M. The validity of inferences based on incomplete observations in disease state models. Biometrics. 1991 Jun;47(2):595-605.\nKlein JP, Moeschberger ML. Survival Analysis: Techniques for Censored and Truncated Data. Springer Science & Business Media; 2005. 538 p.\nvan den Hout A. Multi-State Survival Models for Interval-Censored Data. CRC Press; 2016. 238 p.\nYoung GA, Smith RL. Essentials of Statistical Inference. Cambridge University Press; 2005.\nZhao H, Li Y, Sun J. Analyzing panel count data with a dependent observation process and a terminal event. Can J Stat. 2013 Mar;41(1):174-91.\n\n\n← Previous post\nNext post →"
  },
  {
    "objectID": "blog/classical-survival-methods.html",
    "href": "blog/classical-survival-methods.html",
    "title": "Part III: Classical survival methods",
    "section": "",
    "text": "This is part three of a series on statistical methods for analysing time-to-event, or “survival” data."
  },
  {
    "objectID": "blog/classical-survival-methods.html#statistical-inference",
    "href": "blog/classical-survival-methods.html#statistical-inference",
    "title": "Part III: Classical survival methods",
    "section": "Statistical inference",
    "text": "Statistical inference\nStatistical inference is the process of using data to infer the properties of an underlying probability distribution. In the context of survival analysis, we want to estimate the distribution of time-to-event data, which is often incomplete due to censoring or truncation. This is a key challenge in survival analysis, as we need to account for the fact that we may not observe the event of interest for all individuals in our study. For more details about censored and truncated data see the previous post in this series."
  },
  {
    "objectID": "blog/classical-survival-methods.html#likelihood-estimation",
    "href": "blog/classical-survival-methods.html#likelihood-estimation",
    "title": "Part III: Classical survival methods",
    "section": "Likelihood estimation",
    "text": "Likelihood estimation\nThe likelihood function, often termed \\(L\\), is used to connect statistical models to observed data in both the frequentist and Bayesian frameworks. The likelihood is the probability of observing the data, \\(D\\), given the parameters, \\(\\theta\\), of the model and formally expressed as:\n\\[L(\\theta \\mid D) = f(D \\mid \\theta) = \\prod_i f(X_i \\mid \\theta)\\]\nwhere \\(f\\) is the probability density function, and \\(X_i\\) are assumed to be i.i.d observations in the data.\n\nFrequentist vs Bayesian\nIn the ‘frequentist’ framework the parameters of a statistical model are considered to have a fixed ‘true’ unknown value to be estimated, and the data are realisations of random variables. The parameters of the model are estimated by maximising the likelihood to obtain the maximum likelihood estimate (MLE), denoted \\(\\hat{\\theta}\\).\nBayesian methods are an alternative to maximum likelihood estimation, used to estimate the probability distribution of the parameters of the model given the data, known as the ‘posterior distribution’. In the Bayesian framework, all quantities (both observable quantities and parameters) are considered as random variables, with observed data being realisations of these random variables. The posterior distribution, \\(f(\\theta \\mid D)\\), is related to the likelihood function, \\(f(D \\mid \\theta)\\), via Bayes formula:\n\\[ f(\\theta \\mid D) = \\frac{f(D \\mid \\theta)f(\\theta)}{f(D)}\\]\n\n\nCensored and truncated data\nConstructing likelihoods for censored or truncated data requires careful consideration of the information provided by each observation. The previous post in this series provided a definition of censoring and truncation, and the right and left censoring and truncation times.\nThe likelihood components provided by censored observations are:\n\nfor exactly observed data: information on the probability of the event occurring at this time, i.e. \\(f(X)\\)\nfor right-censored observations: information on the survivor function up to a certain time, i.e. \\(S(C_r)\\)\nfor left-censored observations: information on the cumulative incidence up to a certain time, i.e. \\(F(C_l)\\)\nfor interval-censored observations: information on the probability that the event is within this interval, i.e. \\(S(L) - S(R)\\).\n\nAs data may be observed with different levels of censoring or truncation, the likelihood function is constructed from the joint product of each component for each observation \\(i\\):\n\\[L \\propto \\prod_{i} f(X_i) \\prod_{i} S(C_{r,i}) \\prod_{i} F(C_{l,i}) \\prod_i (S(L_i) - S(R_i))\\]\nThe equivalent likelihood components for truncated data provide information on conditional probabilities of events, i.e. \\(f(X)/F(Y_r)\\) for right-truncated observations, where observation is conditional on the event of interest having occurred by truncation time \\(Y_r\\)."
  },
  {
    "objectID": "blog/classical-survival-methods.html#parametric-non-parametric",
    "href": "blog/classical-survival-methods.html#parametric-non-parametric",
    "title": "Part III: Classical survival methods",
    "section": "Parametric and non-parametric models",
    "text": "Parametric and non-parametric models\nThe statistical models used for estimating model parameters may be parametric or non-parametric. In general, parametric models rely on assumptions about the distribution of the underlying survival times (e.g. Weibull, Gamma, etc.), so careful selection of this distribution is needed for valid inference.\nIn contrast, non-parametric models require fewer assumptions about the survival distribution, so can be applied when the functional form of the distribution is not known, but may require more data and not allow for extrapolation beyond the time period of the observed data. Comparisons to non-parametric estimates can be made to assess the fit of a parametric model."
  },
  {
    "objectID": "blog/classical-survival-methods.html#kaplan-meier",
    "href": "blog/classical-survival-methods.html#kaplan-meier",
    "title": "Part III: Classical survival methods",
    "section": "The Kaplan-Meier estimator",
    "text": "The Kaplan-Meier estimator\nThe Kaplan-Meier estimator is a widely-used, non-parametric method to estimate the survivor function, \\(S(t)\\), from censored data, and defined as:\n\\[\\hat{S}(t) = \\prod_{t_k \\leq t}\\left(\\frac{n(t_k) - d(t_k)}{n(t_k)}\\right)\\]\nwhere \\(d(t_k)\\) is the number of events, and \\(n(t_k)\\) is the number of individuals at risk at time \\(t_k \\leq t\\).\nThe Kaplan-Meier estimator is a step function, with jumps at each observed event time, and the size of the jump is the proportion of individuals who experience the event at that time. The Kaplan-Meier estimator is often plotted as a survival curve, with time on the x-axis and the estimated survival probability on the y-axis.\n\n\n\nExample of a Kaplan-Meier survival plot where time is on the x-axis and the estimated survival probability is on the y-axis."
  },
  {
    "objectID": "blog/classical-survival-methods.html#cox-proportional-hazards",
    "href": "blog/classical-survival-methods.html#cox-proportional-hazards",
    "title": "Part III: Classical survival methods",
    "section": "The Cox proportional hazards model",
    "text": "The Cox proportional hazards model\nThe Cox proportional hazards model assumes that the hazards of death for any two individuals are proportional over time, expressed by writing the hazard function conditional on a given set of \\(M\\) constant or time-varying covariates \\(\\boldsymbol{z} = \\{z_{1}\\ldots, z_{M}\\}\\) as:\n\\[h(t \\mid \\boldsymbol{z}) = h^{(0)}(t)\\exp\\left(\\sum_{m=1}^{M}\\beta_{m}z_{m}\\right) \\equiv h^{(0)}(t)\\exp(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{z})\\]\nwhere \\(h^{(0)}(t)\\) is the baseline hazard function, and \\(\\beta_{m}\\) are the regression coefficients. Notably, the ‘hazard ratio’ for the \\(m\\)th covariate is a constant:\n\\[\\frac{h(t \\mid z_m = 1)}{h(t \\mid z_m = 0)} = \\frac{h^{(0)}(t)\\exp(\\beta_m 1)}{h^{(0)}(t)\\exp(\\beta_m 0)} = \\exp(\\beta_{m})\\]\nThe regression coefficients \\(\\beta_{m}\\) are estimated by maximising the partial likelihood \\(L(\\boldsymbol{\\beta})\\):\n\\[L(\\boldsymbol{\\beta}) = \\prod_{i \\in D}\\frac{\\exp(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{z}_{d_i})}{\\sum_{j \\in R_i}\\exp(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{z}_j)}\\]\nwhere \\(D = \\{T_1, T_2, \\ldots, T_n\\}\\) are the set of distinct failure times, \\(R_i\\) is the set of all individuals who are at risk of failure immediately before time \\(T_i\\), \\(\\boldsymbol{z}_{d_i}\\) is the covariate vector for an individual who failed at time \\(T_i\\), and \\(\\boldsymbol{z}_j\\) is the covariate vector for the \\(j\\)th individual at risk at time \\(T_i\\).\n\n\n\n\n\n\nNoteKey takeaways\n\n\n\n\nStatistical inference allows us to estimate survival distributions despite incomplete data from censoring or truncation.\nLikelihood functions connect our statistical models to observed data, with special adaptations for censored and truncated observations.\nParametric models assume a specific distribution for survival times, while non-parametric approaches make fewer assumptions.\nThe Kaplan-Meier estimator is a non-parametric method to estimate the survivor function from censored data.\nThe Cox proportional hazards model relates survival times to explanatory variables through a semi-parametric approach.\nThe proportional hazards assumption means that the ratio of hazards between different groups remains constant over time."
  },
  {
    "objectID": "blog/classical-survival-methods.html#coming-next",
    "href": "blog/classical-survival-methods.html#coming-next",
    "title": "Part III: Classical survival methods",
    "section": "Coming next",
    "text": "Coming next\nIn the next post, I’ll explain how to handle scenarios where multiple possible events can occur, and the challenges this presents for traditional survival methods. I’ll introduce two methods for competing risks data, analogous to the Kaplan-Meier and Cox models described above:\n\nThe Aalen-Johansen estimator - a non-parametric method for estimating the cumulative incidence function.\nThe Fine-Gray model - a regression model for estimating the effect of covariates on the subdistribution hazard."
  },
  {
    "objectID": "blog/classical-survival-methods.html#references",
    "href": "blog/classical-survival-methods.html#references",
    "title": "Part III: Classical survival methods",
    "section": "References",
    "text": "References\n\nAustin PC. A Tutorial on Multilevel Survival Analysis: Methods, Models and Applications. Int Stat Rev. 2017;85(2):185-203.\nCollett D. Modelling Survival Data in Medical Research. Chapman & Hall/CRC Texts in Statistical Science 2023.\nKlein JP, Moeschberger ML. Survival Analysis: Techniques for Censored and Truncated Data.\nWasserman LA. All of statistics. Springer. 2004.\n\n\n← Previous post\nNext post →"
  },
  {
    "objectID": "blog/multi-state-models.html",
    "href": "blog/multi-state-models.html",
    "title": "Part V: Multi-state models",
    "section": "",
    "text": "This is part five of a series on statistical methods for analysing time-to-event, or “survival” data."
  },
  {
    "objectID": "blog/multi-state-models.html#multi-state-survival-models",
    "href": "blog/multi-state-models.html#multi-state-survival-models",
    "title": "Part V: Multi-state models",
    "section": "Multi-state survival models",
    "text": "Multi-state survival models\nWhen disease processes are more complex, or when intermediate events may influence the final outcome, the standard survival methods described so far may be insufficient to explore the effects of treatment on these outcomes. In these cases, multi-state models, which are more flexible and hence better able to investigate the different pathways that patients may experience, can be applied. These models allow for joint estimation of features of the underlying process and the associated hazards of transition for a set of given covariates, and implicitly account for competing risks.\nThis section provides an introduction to the theory of multi-state processes and describes how these relate to continuous-time and discrete-time multi-state models."
  },
  {
    "objectID": "blog/multi-state-models.html#stochastic-processes",
    "href": "blog/multi-state-models.html#stochastic-processes",
    "title": "Part V: Multi-state models",
    "section": "Stochastic processes underlying multi-state models",
    "text": "Stochastic processes underlying multi-state models\nMulti-state models combine statistical inference with the theory of stochastic processes.\n\nDiscrete and continuous parameter processes\nA stochastic process is formally defined as a collection of random variables \\(\\{X(t),\\; t \\in T\\}\\), indexed by a parameter \\(t\\) which varies in a mathematical index set \\(T\\). The variable \\(X(t)\\) is the state of the process at time \\(t\\), and the set \\(\\mathcal{S}\\) of all possible values of \\(X(t)\\) is termed the state space.\nTwo important cases of stochastic processes are discrete parameter processes, when \\(T = \\{\\pm1, \\pm2, \\pm3, \\dots\\}\\), and continuous parameter processes, when \\(T = \\{t:-\\infty &lt; t &lt; \\infty\\}\\). Both may be restricted to the positive domain, in which case the parameter \\(t\\) may be used to represent time, and these processes are termed ‘discrete-time’ and ‘continuous-time’ processes.\n\n\nCounting processes\nIn survival analyses, the focus is typically on observing events as they occur over time, forming a class of stochastic process known as a point process. If the number of events that occur over time are counted, then the stochastic process is instead known as a counting process. Counting processes form the basis for multi-state models, and linkage to the theory of martingales (not discussed here) provides a theoretical framework for these models.\n\n\nMarkov process\nA stochastic process is called a ‘Markov process’ if the future state of a process depends only on the present state, and not on the sequence of events that preceded it. Equivalently, for a Markov process the conditional probability of a future state \\(X(t_{i+j})\\) given \\(X(t_i)\\) is independent of previous states \\(X(t_0), X(t_1), \\dots, X(t_{i-1})\\).\n\n\nTime-homogeneous Markov processes\nMarkov processes are said to be ‘time-homogeneous’ if the probability of transition from one state to the next (termed the transition probability) is independent of the time parameter \\(t\\). Equivalently, for \\(\\Pr(X(t + u) = s \\mid X(t) = r)\\) denoting the conditional probability of a process \\(X(\\cdot)\\) being in state \\(s\\) at time \\(t + u\\), given the process was in state \\(r\\) at time \\(t\\), a Markov process is time-homogeneous if:\n\\[\n\\Pr(X(t+u) = s \\mid X(t) = r) = \\Pr(X(u) = s \\mid X(0) = r)\n\\]\nA well-known time-homogeneous Markov counting process is the Poisson process, where events occur randomly and independently of each other. A homogeneous Poisson process is described by its constant intensity parameter \\(\\lambda &gt; 0\\), which represents the average rate of events that occur per unit time. For any infinitesimal time interval \\([t, t+\\delta]\\), as \\(\\delta \\rightarrow 0\\), the probability of exactly one event occurring in this interval is asymptotically \\(\\lambda\\delta\\).\n\n\nMulti-state processes\nMulti-state processes are discrete-state stochastic processes with a finite state space \\(\\mathcal{S} = \\{1, 2, \\ldots, m\\}\\). In these processes, an individual begins in one state and spends a random, continuously-distributed time in that state before moving to a random next state. Multi-state processes are defined by the initial distribution of the states \\(\\Pr(X(0) = r)\\), and the transition probabilities \\(\\Pr(X(t + u) = s \\mid X(t) = r, \\mathcal{X}_t)\\) from state \\(r\\) to state \\(s\\) during the interval \\((t, t + u]\\), with \\(r,s \\in \\mathcal{S}\\), and where \\(\\mathcal{X}_t\\) represents the history of the process \\(X(\\cdot)\\) up to time \\(t\\).\nFigure 1 shows how a counting process may be built on top of a multi-state process: panel A shows a multi-state process with transitions between two states, and the corresponding counting process for the transition event from State 1 \\(\\rightarrow\\) State 2 is shown in panel B.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Example of transitions between two states in a multi-state process (panel A), and the corresponding counting process for the State 1 → State 2 transition (panel B)."
  },
  {
    "objectID": "blog/multi-state-models.html#multi-state-models",
    "href": "blog/multi-state-models.html#multi-state-models",
    "title": "Part V: Multi-state models",
    "section": "Multi-state models",
    "text": "Multi-state models\nIn a multi-state model, each state represents a different stage or condition that an individual may experience. The transitions between these states are modelled as a multi-state process, with the time spent in each state and the transitions between states being random variables that can be estimated from data.\nMulti-state models are particularly useful in the context of disease progression, as each state in the model can be used to represent a different level of disease severity, with transitions between states representing progression. This may allow for a more nuanced understanding of disease processes, capturing the different pathways that patients may experience."
  },
  {
    "objectID": "blog/multi-state-models.html#continuous-time-msms",
    "href": "blog/multi-state-models.html#continuous-time-msms",
    "title": "Part V: Multi-state models",
    "section": "Continuous-time multi-state models",
    "text": "Continuous-time multi-state models\n\n\n\n\n\n\nFigure 2: Example of a three-state model with transitions between states according to transition intensities \\(\\{q_{r,s} : r,s \\in (1,2,3)\\}\\)\n\n\n\nContinuous-time multi-state models are defined by transition intensities, \\(q_{r,s}(t,\n\\boldsymbol{z}(t))\\), which represent the instantaneous hazard of moving from state \\(r\\) to state \\(s\\) at time \\(t\\), dependent on a set of explanatory, potentially time-varying, covariates \\(\\boldsymbol{z}(t)\\). Defining an individual’s state at time \\(t\\) as \\(X(t)\\) then:\n\\[\nq_{r,s}(t, \\boldsymbol{z}(t)) = \\lim_{\\delta t\\downarrow 0}\\frac{\\Pr(X(t+\\delta t)=s \\mid X(t)=r)}{\\delta t}\n\\]\nFrom the definition of the hazard function, this transition intensity is equivalent to the hazard of transition from one state to another. These transition intensities form a transition intensity matrix \\(\\mathbf{Q}\\), whose rows sum to zero, with diagonal entries defined by:\n\\[q_{r,r}(t, \\boldsymbol{z}(t)) = - \\sum_{s \\neq r} q_{r,s}(t, \\boldsymbol{z}(t))\\]\nFor the example three-state model shown in Figure 2 this matrix would be:\n\\[\n\\mathbf{Q} = \\begin{bmatrix}\n-q_{1,2}-q_{1,3} & q_{1,2} & q_{1,3} \\\\\nq_{2,1} & -q_{2,1}-q_{2,3} & q_{2,3} \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\]\nHere, state 3 is known as an ‘absorbing’ state, as individuals do not leave the state after arrival, while states 1 and 2 are `transient’ states. The transition probability matrix, which contains the probabilities of moving between states within a time-interval of length \\(t\\) is derived from the transition intensity matrix by the matrix exponential:\n\\[\\mathbf{P}(t) = \\exp(t\\mathbf{Q}) = \\sum_{n=0}^{\\infty}\\frac{t^n}{n!}\\mathbf{Q}^n\\]\n\nCovariates\nThe (potentially time-varying) effect of a given set of \\(M\\) covariates, \\(\\boldsymbol{z}(t) = \\{z_1(t), \\ldots, z_M(t)\\}\\), on the transition intensities \\(q_{r,s}(t, \\boldsymbol{z}(t))\\) can be expressed via a proportional hazards regression model. Typically these combine a parametric baseline hazard, \\(q^{(0)}_{r,s}(t)\\), with log-linear regression:\n\\[\\begin{align*}\nq_{r,s}(t, \\boldsymbol{z}(t)) & = q^{(0)}_{r,s}(t)\\exp\\left(\\sum_{m=1}^{M}\\beta_{r,s,m}z_{m}(t)\\right) \\\\\n& = q^{(0)}_{r,s}(t)\\exp\\left(\\boldsymbol{\\beta}_{r,s}^\\mathsf{T}\\boldsymbol{z}(t)\\right)\n\\end{align*}\\]\nwhere \\(\\exp(\\beta_{r,s,m})\\) is the hazard ratio for the \\(m\\)th covariate on the \\(r \\rightarrow s\\) transition.\n\n\nSojourn time\nThe time spent in a state prior to transition is termed the sojourn time, with its mean given by the inverse of the transition intensity for remaining in the state:\n\\[E(T_r) = -\\frac{1}{q_{r,r}}\\]"
  },
  {
    "objectID": "blog/multi-state-models.html#discrete-time-msms",
    "href": "blog/multi-state-models.html#discrete-time-msms",
    "title": "Part V: Multi-state models",
    "section": "Discrete-time multi-state models",
    "text": "Discrete-time multi-state models\nFor population studies which measure duration in discrete time, discrete-time multi-state models may be applied. Since the concept of instantaneous risk does not apply in discrete time, transition probabilities are instead defined which represent the probability of transition from state \\(r\\) to state \\(s\\) in the time interval \\((t_i,t_j]\\), \\(t_j&gt;t_i\\):\n\\[p_{r,s}(t_i, t_j) = \\Pr(X(t_j) = s \\mid X(t_i) = r)\\]"
  },
  {
    "objectID": "blog/multi-state-models.html#semi-hidden",
    "href": "blog/multi-state-models.html#semi-hidden",
    "title": "Part V: Multi-state models",
    "section": "Semi-Markov and hidden Markov models",
    "text": "Semi-Markov and hidden Markov models\nMulti-state models are typically specified to be Markov, i.e. \\(q_{r,s}(t; \\mathcal{X}_t) = q_{r,s}(t)\\), where \\(\\mathcal{X}_t\\) is the observation history of the process up to time \\(t\\). Semi-Markov models relax the Markov assumption, allowing dependence on the time of entry into the current state, i.e. \\(q_{r,s}(t;\\mathcal{X}_t)=q_{r,s}(t-t_r)\\), where \\(t_r\\) is the time of entry into the current state \\(r\\).\nHidden Markov models may also be specified to allow for unobserved, or ‘latent’, states to be included in the model. These can be particularly useful for modelling complex disease processes, where progression of the disease is influenced by unobserved factors and the time to event depends on the current state of the disease (for instance in models of HIV progression); as well as when the disease state is imperfectly observed (for instance to incorporate measurement error).\n\n\n\n\n\n\nNoteKey takeaways\n\n\n\n\nMulti-state models extend standard survival methods by allowing individuals to move between multiple states over time, not just from “alive” to “dead”\nThese models are built on stochastic processes theory, particularly Markov processes, where future states depend only on the present state\nTransition intensities (analogous to hazard rates) describe the instantaneous risk of moving from one state to another\nMulti-state models can incorporate covariates to identify factors that influence transition rates between states\nDifferent model variations (time-homogeneous, semi-Markov, hidden Markov) provide flexibility to model complex disease processes with various assumptions"
  },
  {
    "objectID": "blog/multi-state-models.html#coming-next",
    "href": "blog/multi-state-models.html#coming-next",
    "title": "Part V: Multi-state models",
    "section": "Coming next",
    "text": "Coming next\nIn the next post, I’ll discuss statistical inference methods for multi-state models, including non-parametric estimation with the Aalen-Johansen estimator and parametric approaches using maximum likelihood."
  },
  {
    "objectID": "blog/multi-state-models.html#references",
    "href": "blog/multi-state-models.html#references",
    "title": "Part V: Multi-state models",
    "section": "References",
    "text": "References\n\nAalen O, Borgan O, Gjessing H. Survival and Event History Analysis: A Process Point of View. Springer; 2008. 540 p.\nAndersen PK, Borgan O, Gill RD, Keiding N. Statistical models based on counting processes. Springer; 1996. 784 p.\nAndersen PK, Keiding N. Multi-state models for event history analysis. Stat Methods Med Res. 2002;11(2):91-115.\nChiang CL. An introduction to stochastic processes and their applications. RE Krieger; 1980.\nPutter H, Fiocco M, Geskus RB. Tutorial in biostatistics: competing risks and multi-state models. Stat Med. 2007;26(11):2389-430.\nvan den Hout A. Multi-State Survival Models for Interval-Censored Data. CRC Press; 2016. 238 p.\n\n\n← Previous post\nNext post →"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Peter Kirwan",
    "section": "",
    "text": "I’m a Research Associate at the MRC Biostatistics Unit in Cambridge. My PhD focussed on applications of multi-state models to estimate infectious disease burden, specifically HIV and COVID-19. My current research involves the development of multi-state “back-calculation” models to estimate HIV incidence and undiagnosed HIV prevalence.\nGitHub · ORCiD · Mastodon"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Peter Kirwan",
    "section": "",
    "text": "I’m a Research Associate at the MRC Biostatistics Unit in Cambridge. My PhD focussed on applications of multi-state models to estimate infectious disease burden, specifically HIV and COVID-19. My current research involves the development of multi-state “back-calculation” models to estimate HIV incidence and undiagnosed HIV prevalence.\nGitHub · ORCiD · Mastodon"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Peter Kirwan",
    "section": "Statistical methods for survival data",
    "text": "Statistical methods for survival data\nThis series of blog posts provides an introduction to statistical methods for survival data, with applications for infectious disease modelling and epidemiology. Topics include: classical survival methods, competing risks analysis, and causal inference from observational data.\n\n\n\n\n\n\n\n\n\n\nPart I: Key concepts in infectious disease research\n\n7 min\n\n\nIncidence\n\nPrevalence\n\nSeverity\n\n\n\nAn overview of key concepts, introducing some of the challenges in estimating the burden of infectious disease.\n\n\n\n\n\n\n\n\n\n\n\n\nPart II: Survival analysis\n\n18 min\n\n\nCensoring\n\nTruncation\n\n\n\nAn introduction to survival analysis, time-to-event data, and how we handle censored observations in epidemiological research.\n\n\n\n\n\n\n\n\n\n\n\n\nPart III: Classical survival methods\n\n6 min\n\n\nLikelihood\n\nSurvival models\n\n\n\nExploring two fundamental approaches to analysing survival data: non-parametric Kaplan-Meier estimation and semi-parametric Cox proportional hazards models.\n\n\n\n\n\n\n\n\n\n\n\n\nPart IV: Competing risks survival methods\n\n6 min\n\n\nCompeting risks\n\nStratification\n\n\n\nHow to handle scenarios where multiple possible events can occur, and the challenges this presents for traditional survival methods.\n\n\n\n\n\n\n\n\n\n\n\n\nPart V: Multi-state models\n\n14 min\n\n\nTransition intensity\n\nMarkov property\n\n\n\nSome theory of multi-state models and their applications in epidemiology, including estimating transition intensities and the time spent in a state.\n\n\n\n\n\n\n\n\n\n\n\n\nPart VI: Statistical inference for multi-state models\n\n10 min\n\n\nMulti-state models\n\nStatistical inference\n\n\n\nDetails of two statistical inference methods for multi-state models and how these are well-suited to the investigation of intermittently-observed data.\n\n\n\n\n\n\n\n\n\n\n\n\nPart VII: Key concepts in causal inference\n\n9 min\n\n\nCausal inference\n\nDAGs\n\n\n\nChallenges and methods for inferring causal relationships from observational data, including propensity score matching and instrumental variables.\n\n\n\n\n\n\n\n\n\n\n\n\nPart VIII: Study designs and biases\n\n9 min\n\n\nBias\n\nConfounding\n\n\n\nAn overview of different observational study designs and the potential biases that can arise, including selection bias and confounding.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#recent-publications",
    "href": "index.html#recent-publications",
    "title": "Peter Kirwan",
    "section": "Recent publications",
    "text": "Recent publications\n\n\n2024\nProtection of vaccine boosters and prior infection against mild/asymptomatic and moderate COVID-19 infection in the UK SIREN healthcare worker cohort: October 2023 to March 2024. Journal of Infection.\n\n\n2024\nEffect of second booster vaccinations and prior infection against SARS-CoV-2 in the UK SIREN healthcare worker cohort. The Lancet Regional Health - Europe.\n\n\n2022\nRe-assessing the late HIV diagnosis surveillance definition in the era of increased and frequent testing. HIV Medicine.\n\n\n2022\nTrends in COVID-19 hospital outcomes in England before and after vaccine introduction, a cohort study. Nature Communications.\n\n\nFull list of publications (via Google Scholar)."
  },
  {
    "objectID": "index.html#talks",
    "href": "index.html#talks",
    "title": "Peter Kirwan",
    "section": "Recorded talks",
    "text": "Recorded talks\n\n2024\nMulti-state modelling to estimate infectious disease burden. University of Cambridge, UK."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Peter Kirwan",
    "section": "Contact",
    "text": "Contact\n\n\nFeel free to reach out about research collaborations, speaking opportunities, or general enquiries:\n📧 pdk29 [at] cam [dot] ac [dot] uk\n🐘 @pkirwan@fediscience.org"
  },
  {
    "objectID": "blog/study-designs.html",
    "href": "blog/study-designs.html",
    "title": "Part VIII: Study designs and biases",
    "section": "",
    "text": "This is part eight of a series on statistical methods for analysing time-to-event, or “survival” data.\nIn this final post of this series I’ll describe a number of observational study designs and several potential sources of bias common to these studies."
  },
  {
    "objectID": "blog/study-designs.html#observational-study-designs",
    "href": "blog/study-designs.html#observational-study-designs",
    "title": "Part VIII: Study designs and biases",
    "section": "Observational study designs",
    "text": "Observational study designs\nObservational studies are common in epidemiology and a valuable resource for understanding infectious disease burden. Several different types of observational study design exist, which may be more or less suitable depending on the research question, the need to control for specific biases, and the feasibility of data collection.\n\n\n\n\n\n\nFigure 1: A hierarchy of different types of study designs and the perceived quality of the evidence. Figure from: Bodicoat D.\n\n\n\nTable 1 shows a \\(2 \\times 2\\) table which may be used to group participants in observational studies and to inform the calculation of standard statistical measures.\n\n\n\n\n\nOutcome\n\n\n\n\n\n\n\nYes\nNo\n\n\nExposure\nYes\n\\(A\\)\n\\(B\\)\n\n\n\nNo\n\\(C\\)\n\\(D\\)\n\n\n\nTable 1: A \\(2 \\times 2\\) table of outcomes and exposures.\n\nCross-sectional studies\nCross-sectional studies may be used to assess:\n\nthe prevalence of an outcome, defined as: \\((A+C)/(A+B+C+D)\\)\nthe prevalence of various different exposures, defined as: \\((A+B)/(A+B+C+D)\\)\n\nwithin a population. They provide a snapshot of the population at a single point in time and can be relatively straightforward to conduct using surveillance data.\n\n\nCase-control studies\nCase-control studies are used to investigate the effects of specific exposures on an outcome. In these studies the outcome of interest has already occurred, individuals who have experienced the outcome are termed “cases”, and are compared to individuals without the outcome, known as “controls”. The odds of being exposed are compared in cases vs. controls via the odds ratio: \\[\\frac{AD}{BC}\\]\n\n\nTest-negative case-control studies\nTest-negative case-control studies are a sub-type of case-control study. These studies aim to minimise bias specifically as a result of test-seeking behaviour by selecting only individuals who test for a disease. As for case-control studies, the cases are individuals who test positive, controls are those who test negative, and again the odds of being exposed are compared between cases and controls.\n\n\nCohort studies\nIn cohort studies participants are selected based on their exposure status and followed up over time. The incidence of the outcome of interest is compared between individuals with and without the exposure using the relative risk:\n\\[\n\\frac{A/(A+B)}{C/(C+D)}\n\\]"
  },
  {
    "objectID": "blog/study-designs.html#biases-in-observational-studies",
    "href": "blog/study-designs.html#biases-in-observational-studies",
    "title": "Part VIII: Study designs and biases",
    "section": "Biases in observational studies",
    "text": "Biases in observational studies\nBias can be defined as: any process at any stage of inference which tends to produce results or conclusions that differ systematically from the truth. All of the observational study designs described above can include potential sources of bias, which are often grouped into three broad types: information bias, collider bias, and confounding.\n\nInformation bias\nInformation, or measurement, bias arises due to inaccuracies in data and may be induced by errors or missing information in either the outcome or exposure variables, for example, incorrect coding of a diagnosis. Information bias can be mitigated through careful study design and data collection mechanisms, these could include: double entry of data, and linking information across different systems for validation.\n\n\nCollider bias\nCollider bias, or selection bias, occurs when individuals are selected into an analysis by conditioning on a common “collider” variable (a variable which is influenced by two other variables).\nFigure 2 shows an example of collider bias: people who are regular smokers and people at high risk of severe COVID-19 illness are both more likely to be admitted to hospital. By conditioning on hospitalisation a distorted association between smoking and COVID-19 mortality could be induced, and this bias may occur in either a positive or negative direction. For example: a negative association between smoking and COVID-19 severity was reported by more than one early risk-factor study for COVID-19, with a review concluding this result may have been a result of collider bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Causal diagram showing the effect of collider bias, where conditioning on hospitalisation induces a distorted association between smoking and COVID-19 severity. Conditioned on variables have bounding boxes, distorted associations are shown as dashed lines.\n\n\n\nCollider bias is most easily avoided by not conditioning on the collider, but this may be impractical depending on the nature of data collection. In the context of COVID-19, Griffith et al. have suggested several methods for assessing the sensitivity of model results to collider bias, such as inverse probability weighting. Where information is available, the extent of the bias can also be examined by comparing the profile of selected individuals to the wider population of interest, e.g. whether hospitalised individuals tend to be older or more likely to have comorbidities as compared to the general population.\n\n\nConfounding\nConfounding is related to collider bias, but occurs by not conditioning on an explanatory variable. As a result confounding is sometimes termed latent variable bias. Confounding introduces a “backdoor path”, an additional source of association between the explanatory variable and the outcome, so the measured association cannot be interpreted as a causal effect.\nAn example of confounding is shown in the causal diagram in Figure 3: during vaccine roll-out older people were more likely to receive a COVID-19 vaccine, but also more likely to experience severe COVID-19. If age group is not adjusted for then a distorted association between vaccination and COVID-19 severity may be induced. Confounding can be avoided by ensuring that factors that might be associated with the outcome are measured and adjusted for.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Causal diagram showing the effect of confounding, where not conditioning on age group induces a distorted association between vaccination and COVID-19 severity. Conditioned on variables have bounding boxes, distorted associations are shown as dashed lines.\n\n\n\n\n\nEpidemic phase bias\nEpidemic phase bias occurs in studies with time-varying incidence of infection. This bias is induced by a relationship between the time from infection to symptom onset and an individual’s eventual outcome, e.g. those who go on to die may experience a more rapid onset of symptoms following infection, an average of \\(c\\) days sooner. Since estimates must be conditioned on the observed symptom onset date, rather than the unobserved infection date, this relationship can introduce bias into results when an epidemic is in a mode of growth or decline.\nEpidemic phase bias is corrected by “shifting” the symptom onset date among those who experience the more severe outcome to be \\(c\\) days later, so that the time from infection to symptom onset is uncorrelated with the outcome (the time-to-event of interest remains the same for both outcomes.) As the true value of \\(c\\) is typically unknown, sensitivity analysis with differing values of \\(c\\) can be used to assess the susceptibility of results to this bias.\n\n\n\n\n\n\nNoteKey takeaways\n\n\n\n\nDifferent observational study designs (cross-sectional, case-control, cohort) are suited to different research questions and data collection scenarios\nCross-sectional studies provide snapshots of prevalence at a single time point, while cohort studies follow exposure groups over time\nCase-control studies compare past exposures between those with and without outcomes, using odds ratios to measure associations\nThree main types of bias affect observational studies: information bias (measurement errors), collider bias (selection effects), and confounding (unmeasured common causes)\nEpidemic phase bias is specific to infectious disease studies with time-varying incidence and rapid symptom onset"
  },
  {
    "objectID": "blog/study-designs.html#references",
    "href": "blog/study-designs.html#references",
    "title": "Part VIII: Study designs and biases",
    "section": "References",
    "text": "References\n\nBaker C. Epidemiology: Study Designs. Blacksburg; 2023.\nBodicoat D. Types of Study Designs in Health Research: The Evidence Hierarchy.\nGriffith GJ, Morris TT, Tudball MJ, et al. Collider bias undermines our understanding of COVID-19 disease risk and severity. Nature Communications. 11(1),5749.\nHernán MA, Robins JM. Causal Inference: What If. CRC Press; 2023, 312 pp.\nSeaman SR, Nyberg T, Overton C, Pascall DJ, et al. Adjusting for time of infection or positive test when estimating the risk of a post-infection outcome in an epidemic. Statistical Methods in Medical Research. 2022;31(10):1942-58.\nWenzl T. Smoking and COVID-19 - A review of studies suggesting a protective effect of smoking against COVID-19. Publications Office. 2020.\nWoodward M. Epidemiology: Study Design and Data Analysis. Chapman & Hall/CRC Texts in Statistical Science; 2013.\n\n\n← Previous post"
  },
  {
    "objectID": "blog/key-concepts-infectious-disease.html",
    "href": "blog/key-concepts-infectious-disease.html",
    "title": "Part I: Key concepts in infectious disease research",
    "section": "",
    "text": "This is part one of a series on statistical methods for analysing time-to-event, or “survival” data."
  },
  {
    "objectID": "blog/key-concepts-infectious-disease.html#epidemiological-measures",
    "href": "blog/key-concepts-infectious-disease.html#epidemiological-measures",
    "title": "Part I: Key concepts in infectious disease research",
    "section": "Epidemiological measures",
    "text": "Epidemiological measures\nEpidemiology is the study and analysis of patterns of diseases and determinants of health in a population. In the context of infectious diseases, epidemiologists and statisticians employ several standard measures of disease burden to assess the impact of a disease, and to guide the appropriate public health response. These measures include:\n\nthe incidence of infection, defined as the rate of new infections occurring over a specific period of time, e.g. new cases per 1,000 people per year;\nthe prevalence of infection, defined as the proportion of the population that have the disease at a specific point in time, regardless of when they became infected;\nfor diseases with adverse outcomes, the mortality associated with infection, defined as the incidence of death from the disease.\n\n\nThe \\(R\\) number\nA key measure of the spread of a disease, related to the incidence of infection, is the basic reproduction number, denoted \\(R_0\\). The \\(R_0\\) value measures the expected number of cases directly generated by one case in a population where all individuals are susceptible to infection.\nFor instance, if \\(R_0 = 3\\) for a particular disease then one infected individual will, on average, infect three others. When \\(R_0 &gt; 1\\) an infection will spread in a population, whereas when \\(R_0 &lt; 1\\) the disease will eventually die out.\nImportantly, not every individual within a population will be equally susceptible to infection. Immunity to a pathogen might be acquired through vaccination or recovery from previous infection, both of which can provide a level of protection against future infections. Through widespread vaccination, immunity can be conferred to a large portion of a population, reducing susceptibility and thereby decreasing \\(R_0\\)."
  },
  {
    "objectID": "blog/key-concepts-infectious-disease.html#estimating-burden",
    "href": "blog/key-concepts-infectious-disease.html#estimating-burden",
    "title": "Part I: Key concepts in infectious disease research",
    "section": "Estimating the burden of infectious disease",
    "text": "Estimating the burden of infectious disease\nTo understand how quickly a virus is spreading through a population, and the corresponding burden of disease, timely and robust estimates are required. The incidence of new cases, prevalence of undiagnosed infection, and extent of disease severity can all inform policy makers seeking to implement effective and appropriate public health measures.\n\nIncidence and prevalence\n\n\n\n\n\n\nFigure 1: An example of the prevalence and incidence of HIV in the US over time. Incidence measures new cases over time, while prevalence measures total active cases at a specific point. Figure from: Steward K.\n\n\n\nIncidence of infection and the prevalence of an infectious disease in a population are generally unobserved, with observable information restricted to test outcomes among subsets of a population. Testing processes may themselves be influenced by unobserved parameters, such as an individual’s propensity to test, and the time from infection to symptom presentation (if symptoms present at all).\nTo reconstruct the incidence and prevalence of infection from the available testing data, statistical and mathematical modelling techniques which adjust for sampling biases are employed. These techniques combine information from a range of sources to make valid statistical inference about unobserved incidence and prevalence.\n\n\nSeverity\nTwo common metrics used to understand the severity of infectious diseases and the factors associated with adverse outcomes are the infection fatality risk (IFR) and case fatality risk (CFR):\n\nInfection Fatality Risk (IFR): The percentage of all infected people who die from the disease (including those whose infections were never detected)\nCase Fatality Rate (CFR): The percentage of confirmed cases who die from the disease\n\nSince infections resulting in death are more likely to be recorded, and mild infections (particularly asymptomatic infections) less likely to be recorded, the CFR will almost always be higher than the IFR. Statistical modelling can be used to relate these measures of severity to underlying characteristics collected through case-surveillance data.\n\n\nEffectiveness of interventions\nMonitoring outcomes among a target group after the introduction of a public health intervention allows for the real-world effectiveness of the intervention to be assessed, and changes over time to be detected. For example, effectiveness estimates might consider reductions in the transmission or acquisition of an infection among a treated group compared to an untreated group. Making these estimates requires reliable testing data, recognition of potential biases, and methodologies which can account for these biases.\n\n\nChallenges in estimating infectious disease burden\nThe majority of the data available for infectious disease monitoring relies on observational data, where a population is followed without actively intervening on an exposure in a randomised way. As a result, estimates of infectious disease burden are often considerably complicated by selection bias, which arises when the selected population observed is different from the general population of interest.\n\n\n\n\n\n\nFigure 2: Schematic showing recruitment-based biases in a hypothetical serosurvey. Figure from: Accorsi et al.\n\n\n\nCommon causes of selection bias in observational data include: testing policies (which may vary nationally and sub-nationally), individual health-seeking behaviour, test availability, clinician decision-making, and detection rates of different tests. Inaccuracies in the available case-surveillance data may also arise: at data input, due to incomplete/inaccurate reporting or misdiagnosis; and at data collection, due to insufficient surveillance infrastructure and data processing methods.\nUnless these potential sources of bias are well-understood and accounted for they can affect the validity of estimates. For example, the IFR measure of severity requires complete knowledge of both the number of infections and number of deaths. Particularly in the early stages of a pandemic, when testing is either unavailable or sub-optimal, these quantities may be impossible to measure. The CFR is more straightforward to assess, since confirmed infections are more likely to be recorded, but this measure is highly sensitive to testing practices. This was particularly the case during the initial wave of the COVID-19 pandemic, when biases in case-surveillance led to wide variability in national estimates of the case-fatality rate, from 0.1% to over 25%.\nEven when knowledge of these multi-faceted biases is available, statistical methodologies must be carefully chosen to properly control for the biases.\n\n\n\n\n\n\nNoteKey takeaways\n\n\n\n\nIncidence of infection is the number of people who acquire an infection within a given period, whereas prevalence is the number of people who have the infection at one time.\nThe basic reproduction number, \\(R_0\\), is the expected number of cases directly generated by one case in a population.\nThere are various ways to assess the impact of a disease, these include incidence and prevalence, as well as disease severity and the effectiveness of interventions.\nMeasuring the burden of infectious disease using observational data is difficult and requires a special class of statistical methods."
  },
  {
    "objectID": "blog/key-concepts-infectious-disease.html#coming-next",
    "href": "blog/key-concepts-infectious-disease.html#coming-next",
    "title": "Part I: Key concepts in infectious disease research",
    "section": "Coming next",
    "text": "Coming next\nIn the next post I’ll provide an introduction to survival analysis, time-to-event data, and how we handle censored observations in epidemiological research."
  },
  {
    "objectID": "blog/key-concepts-infectious-disease.html#references",
    "href": "blog/key-concepts-infectious-disease.html#references",
    "title": "Part I: Key concepts in infectious disease research",
    "section": "References",
    "text": "References\n\nAccorsi EK, Qiu X, Rumpler E, et al. How to detect and reduce potential sources of biases in studies of SARS-CoV-2 and COVID-19. Eur J Epidemiol. 2021;36(2):179-96.\nDe Angelis D, Presanis AM, Birrell PJ, et al. Four key challenges in infectious disease modelling using data from multiple sources. Epidemics. 2015;10:83-7.\nLipsitch M, Cohen T, Cooper B, et al. Transmission dynamics and control of severe acute respiratory syndrome. Science. 2003;300(5627):1966-70.\nPerez-Guzman PN, Knock E, Imai N, et al. Epidemiological drivers of transmissibility and severity of SARS-CoV-2 in England. Nat Commun. 2023;14(1):4279.\nSteward K. Incidence vs Prevalence. Immunology & Microbiology from Technology Networks. Technology Networks; 2020.\nWestreich D. Berkson’s bias, selection bias, and missing data. Epidemiology. 2012;23(1):159-64.\nWorld Health Organization. Estimating mortality from COVID-19 (Scientific Brief). World Health Organization; 2020.\n\nFor more details about key concepts in epidemiology and infectious disease research, see:\n\nEpidemiology: Study design and data analysis by Mark Woodward\n\n\n← Back to all posts\nNext post →"
  },
  {
    "objectID": "blog/competing-risks-methods.html",
    "href": "blog/competing-risks-methods.html",
    "title": "Part IV: Competing risks survival methods",
    "section": "",
    "text": "This is part four of a series on statistical methods for analysing time-to-event, or “survival” data."
  },
  {
    "objectID": "blog/competing-risks-methods.html#competing-risks",
    "href": "blog/competing-risks-methods.html#competing-risks",
    "title": "Part IV: Competing risks survival methods",
    "section": "Competing risks",
    "text": "Competing risks\nIn medical cohort studies, multiple study end-points are common (e.g. death, intensive care admission, and discharge). These multiple end-points are known as competing risks and may hinder the event of interest, or modify the chance that this event occurs. Kaplan-Meier and Cox proportional hazards models treat competing risks as censored observations, and do not account for dependencies between events. Hence the assumption of independent times-to-events in these conventional survival analyses is violated in the presence of competing risks.\n\n\n\n\n\n\nFigure 1: An example of competing risks in survival data. In this scenario, patients can experience one of three outcomes: death, discharge, or intensive care admission. Each outcome “competes” with the others, and experiencing one prevents the observation of others.\n\n\n\nTo correctly address competing risks, “competing risks survival models” are available. In these models an individual is observed over time, with several possible events ‘competing’ until one takes place and the individual transitions to the corresponding state. Importantly there is no assumption of independence for the distribution of the time to competing events and censoring can still be appropriately accounted for."
  },
  {
    "objectID": "blog/competing-risks-methods.html#cause-specific-functions",
    "href": "blog/competing-risks-methods.html#cause-specific-functions",
    "title": "Part IV: Competing risks survival methods",
    "section": "Cause-specific survival functions",
    "text": "Cause-specific survival functions\nWe’ll start by defining the cause-specific hazard and cause-specific cumulative incidence functions, which are the key components of competing risks survival analysis.\n\nCause-specific hazard function\nLet \\(T\\) be a random variable for the survival time and \\(C\\) be a random variable for the cause of failure. The cause-specific hazard function for the \\(j\\)th cause, \\(j = \\{1, 2, \\ldots, m\\}\\) is defined by:\n\\[\nh_j(t) = \\lim_{\\delta t \\downarrow 0} \\frac{\\Pr(t \\leq T &lt; t + \\delta t, C=j \\mid T \\geq t)}{\\delta t}\n\\]\nDefining \\(f_j(t)\\) as the cause-specific density function, and \\(S(t)=\\Pr(T \\geq t)\\) as the overall survivor function, the relationship shown in part II of this series still holds in the presence of competing risks:\n\\[\nh_j(t)=\\frac{f_j(t)}{S(t)}\n\\]\n\n\nCause-specific cumulative incidence\nThe cause-specific cumulative incidence function, i.e. the probability of surviving until time \\(t\\) and failure from cause \\(j\\), in the presence of all other risks, is given by:\n\\[\nF_j(t) = \\Pr(T &lt; t, C=j)\n\\]\nfor \\(j=\\{1, 2, \\dots, m\\}\\), with \\(\\Pr(C=j)\\) often written as \\(\\pi_j\\).\nAn expression for \\(F_j(t)\\) in terms of the cause-specific hazard function may be derived using the equation above:\n\\[\n\\begin{aligned}\nh_j(t) &=\\frac{f_j(t)}{S(t)} \\\\\nf_j(t) &=S(t)h_j(t) \\\\\nF_j(t) &=\\int_0^t S(u)h_j(u) du\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "blog/competing-risks-methods.html#aalen-johansen",
    "href": "blog/competing-risks-methods.html#aalen-johansen",
    "title": "Part IV: Competing risks survival methods",
    "section": "Aalen-Johansen estimator",
    "text": "Aalen-Johansen estimator\nThe standard non-parametric estimator of the cause-specific incidence function is the Aalen-Johansen estimator, also described as the ‘multi-state version’ of the Kaplan-Meier estimator.\nFirstly, by the non-parametric Nelson-Aalen estimator of the cause-specific hazard function for cause \\(j\\):\n\\[\n\\hat{h}_j(t) = \\frac{d_{j}(t)}{n(t\\mbox{-})}\n\\]\nwhere \\(d_{j}(t)\\) is the number of deaths due to cause \\(j\\) at time \\(t\\), and \\(n(t\\mbox{-})\\) is the number of individuals at risk just prior to \\(t\\).\n\nWith the Kaplan-Meier estimate of the survivor function defined in part III of this series, \\(\\hat{S}(t)\\), the Aalen-Johansen estimator for the cause-specific cumulative incidence function then follows from the equation above:\n\\[\n\\hat{F}_j(t) = \\sum_{t_k &lt; t} \\hat{S}(t_{k-1})\\frac{d_{j}(t_k)}{n(t_k\\mbox{-})}\n\\]\nfor all times \\(t_k &lt; t\\) where transition events are observed to occur."
  },
  {
    "objectID": "blog/competing-risks-methods.html#fine-gray",
    "href": "blog/competing-risks-methods.html#fine-gray",
    "title": "Part IV: Competing risks survival methods",
    "section": "Fine-Gray proportional hazards model",
    "text": "Fine-Gray proportional hazards model\nAnalogous to the Cox proportional hazards model described in the last post, the Fine-Gray proportional hazards model may be used estimate the hazard of a competing event (termed the sub-distribution hazard) among those yet to experience an event by time \\(t\\). The risk set for the sub-distribution hazard consists of both those who have yet to experience any event, and those who have yet to experience the event of interest, but who have experienced a competing event.\n\nSub-distribution hazard\nThe sub-distribution hazard is therefore defined as the instantaneous risk of experiencing a competing event \\(j\\) given that the individual has not already experienced this event:\n\\[\n\\lambda_j(t)=\\lim_{\\delta t \\downarrow 0}{\\left\\{\\frac{\\Pr\\left(\\left[t \\leq T &lt; t + \\delta t, C=j \\mid (T&gt;t)\\right] \\cup \\left[(T \\le t \\cap C \\neq j)\\right]\\right)}{\\delta t}\\right\\}}\n\\]\nwhere, as before, \\(C\\) is the random variable for the event that occurs.\n\n\nFine-Gray regression model\nFine-Gray regression links the sub-distribution hazard, \\(\\lambda_{j}\\), to the cause-specific cumulative incidence function, \\(F_{j}\\), through the relationship:\n\\[ \\lambda_{j}(t) = - \\frac{d}{dt} \\log{(1-F_{j}(t))} \\]\nAs with the Cox proportional hazards model, a proportional hazards regression model is assumed, where the hazard of cause \\(j\\) at time \\(t\\) for individual \\(i\\) is:\n\\[\n\\lambda_{i,j}(t) = \\exp(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{z}_i)\\lambda^{(0)}_{j}(t)\n\\]\nwith \\(\\lambda^{(0)}_{j}(t)\\) being the baseline sub-distribution hazard for cause \\(j\\). Covariate coefficients are estimated by maximising the weighted partial likelihood \\(L(\\boldsymbol{\\beta})\\):\n\\[\nL(\\boldsymbol{\\beta}) = \\prod_{i \\in\nD}\\frac{\\exp(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{z}_{d_i})}{\\sum_{j \\in\nR_i}w_{i,j}\\exp(\\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{z}_j)}\n\\]\nwhere \\(w_{i,j}\\) are weights which account for the increasing probability of censoring with increasing follow-up time, \\(D = \\{T_1, T_2, \\ldots, T_n\\}\\) are the set of distinct failure times, \\(R_i\\) is the set of all individuals who are at risk of failure immediately before time \\(T_i\\), \\(\\boldsymbol{z}_{d_i}\\) is the covariate vector for an individual who failed at time \\(T_i\\), and \\(\\boldsymbol{z}_j\\) is the covariate vector for the \\(j\\)th individual at risk at time \\(T_i\\). Covariate effects on the sub-distribution hazard may be interpreted as covariate effects on the cumulative incidence of a competing event."
  },
  {
    "objectID": "blog/competing-risks-methods.html#stratification",
    "href": "blog/competing-risks-methods.html#stratification",
    "title": "Part IV: Competing risks survival methods",
    "section": "Stratification",
    "text": "Stratification\nWhilst proportional hazards models are a common method for incorporating covariates in survival analyses, a more straightforward approach is through stratification. In a stratified model the population is subdivided according to covariate group (or strata), the survival is compared within each stratum, and the differences within stratum are combined to give an overall comparison.\nAs stratification allows the baseline hazard to vary across strata it is sometimes used to accommodate non-proportional hazards in Cox and Fine-Gray proportional hazards.\n\n\n\n\n\n\nNoteKey takeaways\n\n\n\n\nCompeting risks occur when individuals can experience multiple possible events that prevent the observation of other events.\nStandard survival methods (Kaplan-Meier, Cox) can produce biased estimates when competing risks are present.\nThe cause-specific hazard function measures the instantaneous risk of a specific event type.\nThe cause-specific cumulative incidence function gives the probability of experiencing a specific event type over time.\nThe Aalen-Johansen estimator is the non-parametric method for estimating cumulative incidence in competing risks scenarios.\nThe Fine-Gray model allows regression analysis for competing risks data by modelling the sub-distribution hazard.\nStratification can accommodate non-proportional hazards in both Cox and Fine-Gray models."
  },
  {
    "objectID": "blog/competing-risks-methods.html#coming-next",
    "href": "blog/competing-risks-methods.html#coming-next",
    "title": "Part IV: Competing risks survival methods",
    "section": "Coming next",
    "text": "Coming next\nThe competing risks models shown in this post are a special case of multi-state models. In the next post, I’ll provide some of the theory of multi-state models and their applications in epidemiology, including estimating transition intensities and future states."
  },
  {
    "objectID": "blog/competing-risks-methods.html#references",
    "href": "blog/competing-risks-methods.html#references",
    "title": "Part IV: Competing risks survival methods",
    "section": "References",
    "text": "References\n\nAalen OO, Johansen S. An Empirical Transition Matrix for Non-Homogeneous Markov Chains Based on Censored Observations. Scand Stat Theory Appl. 1978;5(3):141-50.\nBorgan, Ø. Nelson-Aalen Estimator. Wiley StatsRef: Statistics Reference Online. 2014.\nCollett, D. Modelling Survival Data in Medical Research. Chapman & Hall/CRC Texts in Statistical Science. 2023.\nFine JP, Gray RJ. A proportional hazards model for the subdistribution of a competing risk. J Am Stat Assoc. 1999;94(446):496-509.\nHosmer DW, Lemeshow S. Applied survival analysis: Regression modeling of time to event data. Wiley. 1999.\n\n\n← Previous post\nNext post →"
  },
  {
    "objectID": "blog/survival-analysis.html",
    "href": "blog/survival-analysis.html",
    "title": "Part II: Survival analysis",
    "section": "",
    "text": "This is part two of a series on statistical methods for analysing time-to-event, or “survival” data."
  },
  {
    "objectID": "blog/survival-analysis.html#what-is-survival-analysis",
    "href": "blog/survival-analysis.html#what-is-survival-analysis",
    "title": "Part II: Survival analysis",
    "section": "What is survival analysis?",
    "text": "What is survival analysis?\nSurvival analysis involves studying “time-to-event” data, also termed “survival data” - a powerful statistical framework that helps us answer questions about when events occur. It applies to any scenario where we’re interested in the time from a given origin to the occurrence of an event (the endpoint).\nIn infectious disease research, survival data are commonly used to describe clinical origins and endpoints, for example:\n\nTime from study recruitment to infection\nDuration from hospitalisation to discharge\nPeriod from infection until recovery or death\n\nThese data may be combined with information on patient characteristics (age, gender, socioeconomic status) and clinical variables (vaccination status, treatments). Survival analysis techniques help us to understand how these factors influence outcomes."
  },
  {
    "objectID": "blog/survival-analysis.html#incomplete-information",
    "href": "blog/survival-analysis.html#incomplete-information",
    "title": "Part II: Survival analysis",
    "section": "Incomplete information",
    "text": "Incomplete information\nOne of the most distinctive aspects of survival analysis is how it handles incomplete information. In real-world studies, we often don’t observe the complete “story” for each participant. This incompleteness comes in several forms:\n\nCensoring\nCensoring occurs when information about an individual is only known within certain intervals or “censoring times”.\n\nRight-censoring\nThe most common type of censoring occurs when our study ends before we observe the event of interest. For example, if we’re studying time to COVID-19 infection, and a participant remains uninfected when our study concludes, they’re “right-censored” - we know they remained infection-free for at least the duration of the study, but we don’t know what happens afterward.\nMore formally, we lack information to the “right” (or future) of the right-censoring time, \\(C_r\\). In this scenario we say an individual is right-censored at time \\(C_r &lt; X\\), where \\(X\\) is the time that the event of interest takes place.\n\n\nLeft-censoring\nIf, on the other hand, the event of interest occurs at an unknown time, \\(X\\), before we started observing the participant at time \\(C_l\\) this is known as “left-censoring”. For instance, if we’re studying HIV infection, and a participant tests positive at their first visit, we know they acquired HIV at some point before joining the study, but we don’t know exactly when. In this scenario we say the individual is left-censored at time \\(C_l &gt; X\\).\n\n\nAdministrative censoring\nSometimes we impose censoring on our dataset, limiting the time-to-event data for each individual to a pre-specified cut-off, with any events beyond this period not considered, e.g. mortality within 30 days of hospital admission. This is known as administrative censoring.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Examples of right, left, and administrative censoring for patients in time-to-event data with origin and outcome information reported. \\(C_l\\): left-censoring time, \\(C_r\\): right-censoring time, \\(\\bullet\\): reported origin, outcome, or intermediate event.\n\n\n\n\n\nInterval censoring\nOften, we only know that an event happened between two observation points, i.e. within a censoring interval \\((L, R],\\ L &lt; X &lt; R\\). An example is the infection time for an individual which is typically not directly observed, but we can assume it occurs in-between a negative and positive test. Interval censoring is a feature of “intermittently-observed” data, where individuals are tested for the presence of infection at several time-points. Such interval-censored data can still be used to detect changes in an individual’s infection status when testing is sufficiently frequent, as shown in Figure 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Example of interval censoring with intermittently-observed data for an underlying process.\n\n\n\n\n\n\nTruncation\nWhile censoring gives us partial information, truncation occurs when information about an individual is completely unobserved, and unavailable at the time of data collection or analysis. This is different from censoring because censored individuals are at least partially observed, while truncated individuals never enter our dataset. As with censoring, both left, right, and interval-truncation are possible.\n\nLeft truncation: occurs when only event-times which take place after the left-truncation time, \\(Y_l\\), are available. For example, if individuals whose infection occurs prior to the study, \\(X &lt; Y_l\\), are not included in our dataset.\nRight-truncation: occurs when only event-times which take place before the right-truncation time, \\(Y_r\\), are available. For example, when we have no knowledge of individuals who are at risk but whose event of interest takes place after the study endpoint, \\(X &gt; Y_r\\).\nInterval-truncation: occurs when only event-times which take place within a specific truncation interval \\([Y_l, Y_r]\\) are available for observation. The key difference from left- and right-truncation is that in interval-truncation, both the left and right boundaries of the observation window play a role in determining which event-times are observed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Examples of left and right-truncation for patients in time-to-event data, assuming only information within the truncation interval (\\(Y_l\\) ,\\(Y_r\\)) is observed. \\(Y_l\\): left-truncation time, \\(Y_r\\): right-truncation time, \\(\\bullet\\): outcome time."
  },
  {
    "objectID": "blog/survival-analysis.html#key-concepts",
    "href": "blog/survival-analysis.html#key-concepts",
    "title": "Part II: Survival analysis",
    "section": "When standard statistics aren’t enough",
    "text": "When standard statistics aren’t enough\nWhy can’t we just use regular statistical methods like t-tests or linear regression for time-to-event data? The key challenge is that censoring and truncation create a form of missing data that’s not random. If we removed all censored observations or treated censoring times as event times, we’d introduce severe bias.\nSurvival analysis methods are specifically designed for valid statistical estimation of incomplete data while avoiding these biases. We’ll start off by defining several fundamental functions for survival analysis:\n\nThe cumulative incidence function\nLet \\(T\\) be the independent and identically distributed (i.i.d) random variable representing the survival time, \\(T = t &gt; 0\\), for an individual, and assume this random variable has a probability distribution with probability density function \\(f(t)\\).\nThe distribution function of \\(T\\), also known as the cumulative incidence function, is the probability of “failure” before time \\(t\\), defined as:\n\\[F(t) = \\Pr(T &lt; t)=\\int_0^t f(u) du\\]\n\n\nThe survivor function\nThe survivor function \\(S(t)\\) gives the probability of surviving (not experiencing the event) beyond time \\(t\\). This is the cornerstone of survival analysis and is defined as:\n\\[S(t) = \\Pr(T \\geq t) = 1 - F(t)\\]\nwhere \\(T\\) is the random variable representing the time until the event occurs.\nThe survivor function starts at 1 (everyone is “alive” at time 0) and decreases toward 0 as time progresses. It has an intuitive interpretation: \\(S(6 \\text{ months}) = 0.8\\) means 80% of individuals remain event-free after 6 months.\n\n\nThe hazard function\nThe hazard function \\(h(t)\\) represents the instantaneous rate of experiencing the event at time \\(t\\), given survival up to that point. Think of it as the “risk” at each moment, conditional on having survived so far:\n\\[h(t) = \\lim_{\\delta t \\to 0} \\frac{\\Pr(t \\leq T &lt; t+\\delta t | T \\geq t)}{\\delta t}\\]\nBy conditional probability, \\(\\Pr(A \\mid B) = \\Pr(AB)/\\Pr(B)\\), so the hazard function can also be expressed as:\n\\[\\begin{aligned}\nh(t) &= \\lim_{\\delta t \\downarrow 0} \\left\\{\\frac{\\Pr(t \\leq T &lt; t + \\delta t)}{\\delta t \\Pr(T \\geq t)}\\right\\} \\\\\n&=\\lim_{\\delta t \\downarrow 0} \\left\\{\\frac{F(t + \\delta t) - F(t)}{\\delta t S(t)}\\right\\} \\\\\n&=\\lim_{\\delta t \\downarrow 0} \\left\\{\\frac{F(t + \\delta t) - F(t)}{\\delta t}\\right\\}\\frac{1}{S(t)}\n\\end{aligned}\\]\nThis limit is the definition of the derivative of \\(F(t)\\) with respect to \\(t\\), and therefore equal to \\(f(t)\\):\n\\[\\lim_{\\delta t \\downarrow 0} \\left\\{\\frac{F(t + \\delta t) - F(t)}{\\delta t}\\right\\} = \\frac{d}{dt}F(t) = f(t)\\]\nHence the hazard function is related to the survivor function through the relationship:\n\\[h(t) = \\frac{f(t)}{S(t)}\\]\n\n\n\n\n\n\nNoteKey takeaways\n\n\n\n\nSurvival analysis deals with “time-to-event” data, helping us understand when events happen, not just if they happen\nThis approach handles incomplete information through concepts like censoring (when we have partial information) and truncation (when some subjects are never observed)\nStandard statistical methods can’t properly handle censored data, which is why specialised survival analysis techniques are essential\nThe survivor function \\(S(t)\\) gives the probability of not experiencing the event beyond time \\(t\\)\nThe hazard function \\(h(t)\\) represents the instantaneous risk of the event at time \\(t\\), given survival up to that point"
  },
  {
    "objectID": "blog/survival-analysis.html#coming-next",
    "href": "blog/survival-analysis.html#coming-next",
    "title": "Part II: Survival analysis",
    "section": "Coming next",
    "text": "Coming next\nIn the next post, I’ll introduce the likelihood function and explore two classical methods for analysing survival data:\n\nThe Kaplan-Meier estimator: a non-parametric approach to estimate survival probabilities;\nThe Cox proportional hazards model: a semi-parametric regression technique for examining covariate effects."
  },
  {
    "objectID": "blog/survival-analysis.html#references",
    "href": "blog/survival-analysis.html#references",
    "title": "Part II: Survival analysis",
    "section": "References",
    "text": "References\n\nCollett D. Modelling Survival Data in Medical Research. Chapman & Hall/CRC Texts in Statistical Science 2023.\nKlein JP, Moeschberger ML. Survival Analysis: Techniques for Censored and Truncated Data.\n\n\n← Previous post\nNext post →"
  }
]